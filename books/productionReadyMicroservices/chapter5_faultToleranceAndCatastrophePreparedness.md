# Fault Tolerance and Catastrophe Preparedness

- check list
  - it has no single point of failure
  - all failure scenarios and possible catastrophes have been identified
  it is tested for resiliency through code testing, load testing, and chaos testing
  - failure detection and remediation has been automated
  - there are standardized incident and outage procedures in place within microservice development team and across the organization
- no microservice is immune to production fires
- identify and architect away single points of failure
- identify failure scenarios
  - internal
    - ex: host, logic failures
  - external
    - ex: queue, datacenter, other microservice failures
- test for these failures
  - code testing
  - load testing
  - chaos testing
  - it is crucial that this failure testing also happens in production
  - any new failures that occur need to be added to this test suite
- failure detection and mitigation need to be departmentally standardized
- single points of failure
  - they can affect entire ecosystems (even unrelated services) if you aren't careful
  - task processor breaks -> queue builds up -> queue's machine runs out of memory -> everything on that machine dies
  - identifying: draw out your architecture and ask the question "what happens if this fails?" for each piece
- common failure causes
  - insufficient design reviews
  - incomplete code reviews
  - poor development process
  - unstable deployment procedure
- common hardware failures
  - host
  - rack
  - datacenter
  - cloud provider
  - server provisioning
  - resource isolation and/or abstraction technology failure
  - broken config management
  - failures caused by config changes
  - failures and gaps in host-level monitoring
  - failures and gaps in host-level logging
  - network failure
  - operational downtime
  - lack of infrastructure redundancy
- common communication and application failures
  - network
  - DNS
  - RPC
  - improper handling of requests/responses
  - messaging system failures
  - failures in service discovery and service registry
  - improper load balancing
  - failure of development tools and development env
  - failures in the test, package, build, and release pipelines
  - deployment pipeline failures
  - failures and gaps in microservice-level logging
  - failures and gaps in microservice-level monitoring
- common dependency failures
  - downstream microservice failurs
  - internal service outages
  - external (third-party) service outages
  - internal library failures
  - external (third-party) library failures
  - dependency failing to meet its SLA
    - the SLA of a service can only be as good as the product of all of the uptime percentages of downstream dependencies
  - api endpoint deprecation
  - api endpoint decommissioning
  - microservice deprecation
  - microservice decommissioning
  - interface or endpoint deprecation
  - timeouts to a downstream service
  - timeoutes to an external dependency
- common internal failures
  - this is the most common scenario that a microservice team would be concerned with
  - incomplete code reviews
  - poor architecture and design
  - lack of proper unit and integration tests
  - bad deployments
  - lack of proper monitoring
  - improper error and exception handling
  - db failure
  - scalability limitations
- resiliency testing
  - code testing
    - lint
    - unit
    - integration
    - e2e
    - automating tests
  - load testing
    - it can be used to verify that a microservice meets a specification for load
    - it can also be used to find what a microservice can handle so that specifications for load can be set
    - good for staging environments
    - still need to load test prod tho
    - alert downstream dependencies
    - this needs standardization if being done department wide
    - scheduled regularly
    - test during periods of low trafic **never during peak hours**
    - target traffic loads are calculated using qualitative and quantitative growth scales
    - runs at each stage of the deployment pipeline
    - it is logged
    - it is automated
  - chaos testing
    - logging is critical in case things go rogue
    - expect teams to run standard tests
    - give teams the abilities to create new custom ones too
    - examples
      - disable the api endpoint of one of a microservice's deps
      - stop all traffic requests to a dep
      - introduce latency between various parts of the ecosystem to mimic network problems
        - between clients and deps
        - between microservices and shared dbs
        - between microservices and distrib task processinc systems
        - etc
      - stop all traffic to a datacenter or a region
      - take out a host at random by shutting down one machine
- failure detection and remediation
  - number 1 goal: reduce impact on users
  - rolling back changes (even low level ones) is one good strategy
  - failing over to stable alternative is good too
    - like rerouting traffic to a different datacenter
  - **requires monitoring**
  - automating mitigation strategies reduces the human error chance (prob faster too)
- incidents and outages
  - categorization
    - categorizing microservices
      - based on impact to overall ecosystem and criticality to business
      - infrastructure and application platform layers are always of highest criticality
    - categorizing indicents
      - so everyone knows the severity of a failure
      - two axes: severity and scope
      - severity is linked to the categorization of the application, microservice, or system in question
      - scope relates to how much of the ecosystem is affected
- incident response stages
  1. assessment
     - needs monitoring
     - for determining the severity and scope
  2. coordination
     - communication with other developers and teams
     - need clear channels for this
  3. mitigation
     - first worry about users/clients
     - this isn't resolution
     - issues are mitigated when both its availability and availability of its clients are no longer compromised
  4. resolution
     - this is when the clock stops ticking
  5. follow up
     - postmortems need to be made
       - most effective when blameless
     - learning what happened
     - plans of action for how to avoid it in the future
     - 